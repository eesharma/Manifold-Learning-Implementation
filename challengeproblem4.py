# -*- coding: utf-8 -*-
"""ChallengeProblem4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JEKnibDXbeGuUgbm9iswnhjiHU645XGy

# VS 265: Manifold Learning




by: Eeshan Sharma
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from scipy.linalg import lstsq
import heapdict

"""# Background

In general, high-dimensional data sets are difficult to visualize. Manifold learning offers dimensional reduction techniques that can effectively visualize datasets by projecting onto 2D spaces. It can be thought of as an attempt to generalize linear frameworks to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.

**Locally linear embedding (LLE)** seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. The algorithm, as a subset of Manifold Learning, attempts to uncover non-linear relationships hidden in the data. The process is as follows: 

1. **Nearest Neighbors Search**
2. **Weight Matrix Consideration**: The construction of the LLE weight matrix involves the solution of a  linear equation for each of the  local neighborhoods.
3. **Partial Eigenvalue Decomposition**

I will use the MNIST dataset and use the number "4" as the input data for the LLE algorithm.

# Implementation
"""

(X_train, y_train), (X_test, y_test) = mnist.load_data()
X = [X_train[i] for i in range(len(X_train)) if y_train[i] == 4][:1000]
plt.imshow(X[0])
print(X[0].shape)

"""First, find the "nearest neighbors" for an individual data point. This is step 1 in the LLE implementation. """

def get_dist(x, y):
  return np.linalg.norm(x-y)

def nearest_k(data_index, X, k):
  point = X[data_index]
  distances = {}
  closest = []
  for i in range(len(X)):
    if i != data_index:
      distances[i] = get_dist(point, X[i])
  for p in sorted(distances, key=distances.get):
    if len(closest) == k:
      return closest
    closest.append(p)

def filter_1(x, y):
  keep = (y == 1)
  x, y = x[keep], y[keep]
  y = y == 1
  return x,y

"""Now, calculate the weights that will function to assign a weight to each neighbor. This is done by minimizing the mean squared error of the representation of X with the observed value of X. This is step 2 in LLE implementation. """

def weights_calc(data_index, neighbors, X):
  Y = []
  for n_index in neighbors:
    neighbor = X[n_index]
    Y.append(neighbor.flatten() - X[data_index].flatten())
  Y = np.array(Y)
  C = np.matmul(Y, Y.T)
  weights = np.linalg.solve(C, np.ones(len(C)).T)
  
  total_sum = sum(weights)
  weights_normal = [w / total_sum for w in weights]
  
  return weights_normal

"""To project the weight matrix on a 2D space, first calculate the embedding in a lower dimension. Then, solve the minimum eigenvalue problem, and then implement the final helper function of the LLE algorithm. This is step 3 in the LLE implementation. """

def embedding(neighbors, weights, X, d):
  temp_weights = np.identity(len(weights)) - weights
  M = np.matmul(temp_weights.T, temp_weights)
  eigenvalues, eigenvectors = np.linalg.eig(M)
  eigeninfo = {}
  lowest_eigenvectors = []
  for i in range(len(eigenvalues)):
    eigeninfo[eigenvalues[i]] = eigenvectors[i]
  for key in sorted(eigeninfo):
    if len(lowest_eigenvectors) == d + 1:
      break
    lowest_eigenvectors.append(eigeninfo[key])
  return np.array(lowest_eigenvectors).T


def LLE(X, k, d):
  neighbors = []
  weights = []
  
  for i in range(len(X)):
    curr_neighbors = nearest_k(i, X, k)
    neighbors.append(curr_neighbors)
    curr_weights = np.zeros(len(X))
    neighbor_weights = weights_calc(i, curr_neighbors, X)
  
    for i in range(k):
      curr_weights[curr_neighbors[i]] = neighbor_weights[i]
    weights.append(curr_weights)

  #graph the embedded representation
  new_representation = embedding(neighbors, weights, X, d)
  x_embedding = new_representation[:,0]
  y_embedding = new_representation[:,1]
  plt.scatter(x_embedding, y_embedding)
  plt.title("k = " + str(k))
  plt.show()

for k in [10, 20, 30]:
  LLE(X, k, 2)

"""We can see in the above 3 plots that there is a singular cluster with a majority of the data points, with a small number of outliers on all three graphs towards the extreme portions. The number of outliers tends to increase as the value of k increases. 

In the future, I would like to implement different variations of LLE, like Hessian and Modified LLE algorithms. This would be an interesting thing to do; comparing the results of the variants of LLE algorithms to see how they functionally differ could yield some interesting insights, and more importantly, solidify my understanding of the concepts.

# References

https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Hessian_Locally-Linear_Embedding_(Hessian_LLE)

https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding

**For Step 3 of the implementation, I had a lot of trouble running the code without errors, so I used other students' work to model my implementation of the LLE at least for submission. I will play around with this program a little more to see if I can fix some of the errors afterwards. **
"""

